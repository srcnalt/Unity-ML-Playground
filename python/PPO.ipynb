{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unity ML Agents\n",
    "## Proximal Policy Optimization (PPO)\n",
    "Contains an implementation of PPO as described [here](https://arxiv.org/abs/1707.06347)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "from ppo.history import *\n",
    "from ppo.models import *\n",
    "from ppo.trainer import Trainer\n",
    "from unityagents import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### General parameters\n",
    "max_steps = 5e5 # Set maximum number of steps to run environment.\n",
    "run_path = \"ppo\" # The sub-directory name for model and summary statistics\n",
    "load_model = False # Whether to load a saved model.\n",
    "train_model = True # Whether to train the model.\n",
    "summary_freq = 100 # Frequency at which to save training statistics.\n",
    "save_freq = 1000 # Frequency at which to save model.\n",
    "env_name = \"pong\" # Name of the training environment file.\n",
    "curriculum_file = None\n",
    "\n",
    "### Algorithm-specific parameters for tuning\n",
    "gamma = 0.99 # Reward discount rate.\n",
    "lambd = 0.95 # Lambda parameter for GAE.\n",
    "time_horizon = 2048 # How many steps to collect per agent before adding to buffer.\n",
    "beta = 1e-3 # Strength of entropy regularization\n",
    "num_epoch = 5 # Number of gradient descent steps per batch of experiences.\n",
    "num_layers = 2 # Number of hidden layers between state/observation encoding and value/policy layers.\n",
    "epsilon = 0.2 # Acceptable threshold around ratio of old and new policy probabilities.\n",
    "buffer_size = 2048 # How large the experience buffer should be before gradient descent.\n",
    "learning_rate = 3e-4 # Model learning rate.\n",
    "hidden_units = 64 # Number of units in hidden layer.\n",
    "batch_size = 64 # How many experiences per gradient descent update step.\n",
    "normalize = False\n",
    "\n",
    "### Logging dictionary for hyperparameters\n",
    "hyperparameter_dict = {'max_steps':max_steps, 'run_path':run_path, 'env_name':env_name,\n",
    "    'curriculum_file':curriculum_file, 'gamma':gamma, 'lambd':lambd, 'time_horizon':time_horizon,\n",
    "    'beta':beta, 'num_epoch':num_epoch, 'epsilon':epsilon, 'buffe_size':buffer_size,\n",
    "    'leaning_rate':learning_rate, 'hidden_units':hidden_units, 'batch_size':batch_size}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unity Academy name: Academy\n",
      "        Number of brains: 1\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: Brain\n",
      "        Number of observations (per agent): 0\n",
      "        State space type: continuous\n",
      "        State space size (per agent): 3\n",
      "        Action space type: discrete\n",
      "        Action space size (per agent): 3\n",
      "        Memory space size (per agent): 0\n",
      "        Action descriptions: , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=env_name, curriculum=curriculum_file)\n",
    "print(str(env))\n",
    "brain_name = env.external_brain_names[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Agent(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100. Mean Reward: 1.0. Std of Reward: 0.0.\n",
      "Step: 200. Mean Reward: -0.5. Std of Reward: 0.408248290463863.\n",
      "Step: 300. Mean Reward: 0.0. Std of Reward: 1.0.\n",
      "Step: 400. Mean Reward: -0.5. Std of Reward: 0.0.\n",
      "Step: 500. Mean Reward: -0.625. Std of Reward: 0.414578098794425.\n",
      "Step: 600. Mean Reward: -0.9166666666666666. Std of Reward: 0.18633899812498245.\n",
      "Step: 700. Mean Reward: -0.625. Std of Reward: 0.414578098794425.\n",
      "Step: 800. Mean Reward: -0.5. Std of Reward: 0.7071067811865476.\n",
      "Step: 900. Mean Reward: -0.125. Std of Reward: 0.739509972887452.\n",
      "Step: 1000. Mean Reward: -0.5. Std of Reward: 0.408248290463863.\n",
      "Saved Model\n",
      "Step: 1100. Mean Reward: 1.0. Std of Reward: 0.0.\n",
      "Step: 1200. Mean Reward: -0.625. Std of Reward: 0.414578098794425.\n",
      "Step: 1300. Mean Reward: -0.3333333333333333. Std of Reward: 0.9428090415820634.\n",
      "Step: 1400. Mean Reward: -0.7. Std of Reward: 0.6.\n",
      "Step: 1500. Mean Reward: 0.0. Std of Reward: 1.0.\n",
      "Step: 1600. Mean Reward: -0.5. Std of Reward: 0.408248290463863.\n",
      "Step: 1700. Mean Reward: 1.0. Std of Reward: 0.0.\n",
      "Step: 1800. Mean Reward: -0.6428571428571429. Std of Reward: 0.6925256939166183.\n",
      "Step: 1900. Mean Reward: -0.3333333333333333. Std of Reward: 0.6236095644623235.\n",
      "Step: 2000. Mean Reward: 0.0. Std of Reward: 1.0.\n",
      "Saved Model\n",
      "Step: 2100. Mean Reward: -0.625. Std of Reward: 0.649519052838329.\n",
      "Step: 2200. Mean Reward: -0.5. Std of Reward: 0.7071067811865476.\n",
      "Step: 2300. Mean Reward: -0.3333333333333333. Std of Reward: 0.9428090415820634.\n",
      "Step: 2400. Mean Reward: 1.0. Std of Reward: 0.0.\n",
      "Step: 2500. Mean Reward: 0.0. Std of Reward: 1.0.\n",
      "Step: 2600. Mean Reward: -0.75. Std of Reward: 0.3818813079129867.\n",
      "Step: 2700. Mean Reward: -0.25. Std of Reward: 0.5590169943749475.\n",
      "Step: 2800. Mean Reward: -0.75. Std of Reward: 0.5590169943749475.\n",
      "Step: 2900. Mean Reward: -0.875. Std of Reward: 0.21650635094610965.\n",
      "Step: 3000. Mean Reward: -0.5. Std of Reward: 0.408248290463863.\n",
      "Saved Model\n",
      "Step: 3100. Mean Reward: -0.625. Std of Reward: 0.649519052838329.\n",
      "Step: 3200. Mean Reward: -0.7. Std of Reward: 0.6000000000000001.\n",
      "Step: 3300. Mean Reward: -0.5. Std of Reward: 0.7071067811865476.\n",
      "Step: 3400. Mean Reward: -0.25. Std of Reward: 0.25.\n",
      "Step: 3500. Mean Reward: 1.0. Std of Reward: 0.0.\n",
      "Step: 3600. Mean Reward: -0.08333333333333333. Std of Reward: 0.9316949906249122.\n",
      "Step: 3700. Mean Reward: -0.8333333333333334. Std of Reward: 0.23570226039551584.\n",
      "Step: 3800. Mean Reward: -0.625. Std of Reward: 0.414578098794425.\n",
      "Step: 3900. Mean Reward: -0.5. Std of Reward: 0.408248290463863.\n",
      "Step: 4000. Mean Reward: -0.6666666666666666. Std of Reward: 0.5527707983925666.\n",
      "Saved Model\n",
      "Step: 4100. Mean Reward: -0.75. Std of Reward: 0.5590169943749475.\n",
      "Step: 4200. Mean Reward: 0.0. Std of Reward: 0.5.\n",
      "Step: 4300. Mean Reward: -0.7. Std of Reward: 0.6000000000000001.\n",
      "Step: 4400. Mean Reward: 0.0. Std of Reward: 0.5.\n",
      "Step: 4500. Mean Reward: 0.75. Std of Reward: 0.25.\n",
      "Step: 4600. Mean Reward: -0.625. Std of Reward: 0.414578098794425.\n",
      "Step: 4700. Mean Reward: -0.625. Std of Reward: 0.649519052838329.\n",
      "Step: 4800. Mean Reward: -0.875. Std of Reward: 0.21650635094610965.\n",
      "Step: 4900. Mean Reward: -0.25. Std of Reward: 0.25.\n",
      "Step: 5000. Mean Reward: -0.9166666666666666. Std of Reward: 0.18633899812498245.\n",
      "Saved Model\n",
      "Step: 5100. Mean Reward: -0.5. Std of Reward: 0.7071067811865476.\n",
      "Step: 5200. Mean Reward: 1.0. Std of Reward: 0.0.\n",
      "Step: 5300. Mean Reward: -0.3333333333333333. Std of Reward: 0.6236095644623235.\n",
      "Step: 5400. Mean Reward: 1.0. Std of Reward: 0.0.\n",
      "Step: 5500. Mean Reward: 0.0. Std of Reward: 1.0.\n",
      "Step: 5600. Mean Reward: -0.8571428571428571. Std of Reward: 0.3499271061118826.\n",
      "Step: 5700. Mean Reward: -0.875. Std of Reward: 0.21650635094610965.\n",
      "Step: 5800. Mean Reward: -0.5. Std of Reward: 0.8660254037844386.\n",
      "Step: 5900. Mean Reward: -0.8. Std of Reward: 0.4.\n",
      "Step: 6000. Mean Reward: -0.16666666666666666. Std of Reward: 0.4714045207910317.\n",
      "Saved Model\n",
      "Step: 6100. Mean Reward: -0.5. Std of Reward: 0.7071067811865476.\n",
      "Step: 6200. Mean Reward: 0.0. Std of Reward: 0.5.\n",
      "Step: 6300. Mean Reward: 0.0. Std of Reward: 0.816496580927726.\n",
      "Step: 6400. Mean Reward: -0.625. Std of Reward: 0.414578098794425.\n",
      "Step: 6500. Mean Reward: -0.7. Std of Reward: 0.6.\n",
      "Step: 6600. Mean Reward: -1.0. Std of Reward: 0.0.\n",
      "Step: 6700. Mean Reward: -0.3333333333333333. Std of Reward: 0.6236095644623235.\n",
      "Step: 6800. Mean Reward: -0.75. Std of Reward: 0.5590169943749475.\n",
      "Step: 6900. Mean Reward: -0.8333333333333334. Std of Reward: 0.37267799624996495.\n",
      "Step: 7000. Mean Reward: -0.3333333333333333. Std of Reward: 0.9428090415820634.\n",
      "Saved Model\n",
      "Step: 7100. Mean Reward: 0.25. Std of Reward: 0.75.\n",
      "Step: 7200. Mean Reward: -0.125. Std of Reward: 0.649519052838329.\n",
      "Step: 7300. Mean Reward: -0.625. Std of Reward: 0.649519052838329.\n",
      "Step: 7400. Mean Reward: -0.8333333333333334. Std of Reward: 0.23570226039551584.\n",
      "Step: 7500. Mean Reward: -0.5. Std of Reward: 0.6123724356957945.\n",
      "Step: 7600. Mean Reward: 0.0. Std of Reward: 1.0.\n",
      "Step: 7700. Mean Reward: 1.0. Std of Reward: 0.0.\n",
      "Step: 7800. Mean Reward: 1.0. Std of Reward: 0.0.\n",
      "Step: 7900. Mean Reward: 0.0. Std of Reward: 0.5.\n",
      "Step: 8000. Mean Reward: -0.75. Std of Reward: 0.25.\n",
      "Saved Model\n",
      "Step: 8100. Mean Reward: -0.3. Std of Reward: 0.7483314773547882.\n",
      "Step: 8200. Mean Reward: 0.0. Std of Reward: 0.5.\n",
      "Step: 8300. Mean Reward: -0.5. Std of Reward: 0.8660254037844386.\n",
      "Step: 8400. Mean Reward: -0.625. Std of Reward: 0.414578098794425.\n",
      "Step: 8500. Mean Reward: -0.625. Std of Reward: 0.414578098794425.\n",
      "Step: 8600. Mean Reward: -0.5. Std of Reward: 0.408248290463863.\n",
      "Step: 8700. Mean Reward: -0.5. Std of Reward: 0.3535533905932738.\n",
      "Step: 8800. Mean Reward: -0.6666666666666666. Std of Reward: 0.4714045207910317.\n",
      "Step: 8900. Mean Reward: -0.16666666666666666. Std of Reward: 0.8498365855987975.\n",
      "Step: 9000. Mean Reward: -0.5. Std of Reward: 0.7071067811865476.\n",
      "Saved Model\n",
      "Step: 9100. Mean Reward: -0.3333333333333333. Std of Reward: 0.6236095644623235.\n",
      "Step: 9200. Mean Reward: -0.7. Std of Reward: 0.6.\n",
      "Step: 9300. Mean Reward: 0.0. Std of Reward: 0.5.\n",
      "Step: 9400. Mean Reward: -0.75. Std of Reward: 0.3818813079129867.\n",
      "Step: 9500. Mean Reward: 0.5. Std of Reward: 0.0.\n",
      "Step: 9600. Mean Reward: -0.5. Std of Reward: 0.6123724356957945.\n",
      "Step: 9700. Mean Reward: 0.0. Std of Reward: 1.0.\n",
      "Step: 9800. Mean Reward: -0.5. Std of Reward: 0.6123724356957945.\n",
      "Step: 9900. Mean Reward: 0.75. Std of Reward: 0.25.\n",
      "Step: 10000. Mean Reward: 0.16666666666666666. Std of Reward: 1.0274023338281628.\n",
      "Saved Model\n",
      "Step: 10100. Mean Reward: -0.3333333333333333. Std of Reward: 0.9428090415820634.\n",
      "Step: 10200. Mean Reward: -0.875. Std of Reward: 0.21650635094610965.\n",
      "Step: 10300. Mean Reward: -0.625. Std of Reward: 0.649519052838329.\n",
      "Step: 10400. Mean Reward: -0.625. Std of Reward: 0.414578098794425.\n",
      "Step: 10500. Mean Reward: 1.0. Std of Reward: 0.0.\n",
      "Step: 10600. Mean Reward: -0.625. Std of Reward: 0.414578098794425.\n",
      "Step: 10700. Mean Reward: -0.5. Std of Reward: 0.408248290463863.\n",
      "Step: 10800. Mean Reward: 0.0. Std of Reward: 0.7905694150420949.\n",
      "Step: 10900. Mean Reward: -0.625. Std of Reward: 0.414578098794425.\n",
      "Step: 11000. Mean Reward: -0.3333333333333333. Std of Reward: 0.6236095644623235.\n",
      "Saved Model\n",
      "Step: 11100. Mean Reward: -0.25. Std of Reward: 0.75.\n",
      "Step: 11200. Mean Reward: -0.5. Std of Reward: 0.6123724356957945.\n",
      "Step: 11300. Mean Reward: 0.5. Std of Reward: 0.0.\n",
      "Step: 11400. Mean Reward: -0.3333333333333333. Std of Reward: 0.6236095644623235.\n",
      "Step: 11500. Mean Reward: -0.5. Std of Reward: 0.408248290463863.\n",
      "Step: 11600. Mean Reward: -0.5. Std of Reward: 0.8660254037844386.\n",
      "Step: 11700. Mean Reward: -0.5. Std of Reward: 0.7071067811865476.\n",
      "Step: 11800. Mean Reward: -0.6. Std of Reward: 0.5830951894845301.\n",
      "Step: 11900. Mean Reward: -0.8333333333333334. Std of Reward: 0.23570226039551584.\n",
      "Step: 12000. Mean Reward: -0.625. Std of Reward: 0.414578098794425.\n",
      "Saved Model\n",
      "Step: 12100. Mean Reward: 1.0. Std of Reward: 0.0.\n",
      "Step: 12200. Mean Reward: 0.0. Std of Reward: 1.0.\n",
      "Step: 12300. Mean Reward: -0.3333333333333333. Std of Reward: 0.23570226039551584.\n",
      "Step: 12400. Mean Reward: -0.5. Std of Reward: 0.7071067811865476.\n",
      "Step: 12500. Mean Reward: 0.25. Std of Reward: 0.75.\n",
      "Step: 12600. Mean Reward: -0.125. Std of Reward: 0.649519052838329.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 12700. Mean Reward: 0.0. Std of Reward: 1.0.\n",
      "Step: 12800. Mean Reward: -0.5. Std of Reward: 0.7071067811865476.\n",
      "Step: 12900. Mean Reward: -0.625. Std of Reward: 0.414578098794425.\n",
      "Step: 13000. Mean Reward: 1.0. Std of Reward: 0.0.\n",
      "Saved Model\n",
      "Step: 13100. Mean Reward: -0.875. Std of Reward: 0.21650635094610965.\n",
      "Step: 13200. Mean Reward: 0.0. Std of Reward: 0.5.\n",
      "Step: 13300. Mean Reward: -0.3333333333333333. Std of Reward: 0.6236095644623235.\n",
      "Step: 13400. Mean Reward: 1.0. Std of Reward: 0.0.\n",
      "Step: 13500. Mean Reward: 0.3333333333333333. Std of Reward: 0.9428090415820634.\n",
      "Step: 13600. Mean Reward: -0.8571428571428571. Std of Reward: 0.3499271061118826.\n",
      "Step: 13700. Mean Reward: -0.875. Std of Reward: 0.33071891388307384.\n",
      "Step: 13800. Mean Reward: -0.3333333333333333. Std of Reward: 0.9428090415820634.\n",
      "Step: 13900. Mean Reward: -0.5. Std of Reward: 0.7071067811865476.\n",
      "Step: 14000. Mean Reward: -1.0. Std of Reward: 0.0.\n",
      "Saved Model\n",
      "Step: 14100. Mean Reward: -0.5. Std of Reward: 0.6123724356957945.\n",
      "Step: 14200. Mean Reward: -0.25. Std of Reward: 0.75.\n",
      "Step: 14300. Mean Reward: 1.25. Std of Reward: 0.25.\n",
      "Step: 14400. Mean Reward: 0.16666666666666666. Std of Reward: 0.4714045207910317.\n",
      "Step: 14500. Mean Reward: -0.5. Std of Reward: 0.408248290463863.\n",
      "Step: 14600. Mean Reward: 1.0. Std of Reward: 0.0.\n",
      "Step: 14700. Mean Reward: -0.8333333333333334. Std of Reward: 0.37267799624996495.\n",
      "Step: 14800. Mean Reward: 1.0. Std of Reward: 0.0.\n",
      "Step: 14900. Mean Reward: -0.625. Std of Reward: 0.414578098794425.\n",
      "Step: 15000. Mean Reward: 0.0. Std of Reward: 1.0.\n",
      "Saved Model\n",
      "Step: 15100. Mean Reward: -0.625. Std of Reward: 0.414578098794425.\n",
      "Step: 15200. Mean Reward: 0.25. Std of Reward: 0.75.\n",
      "Step: 15300. Mean Reward: 0.0. Std of Reward: 0.7071067811865476.\n",
      "Step: 15400. Mean Reward: -0.625. Std of Reward: 0.649519052838329.\n",
      "Step: 15500. Mean Reward: -0.875. Std of Reward: 0.21650635094610965.\n",
      "Step: 15600. Mean Reward: -0.5. Std of Reward: 0.408248290463863.\n",
      "Step: 15700. Mean Reward: -0.5. Std of Reward: 0.7071067811865476.\n",
      "Step: 15800. Mean Reward: 0.5. Std of Reward: 0.0.\n",
      "Step: 15900. Mean Reward: -0.5. Std of Reward: 0.408248290463863.\n",
      "Step: 16000. Mean Reward: 0.25. Std of Reward: 0.75.\n",
      "Saved Model\n",
      "Step: 16100. Mean Reward: 0.5. Std of Reward: 0.0.\n",
      "Step: 16200. Mean Reward: -0.3. Std of Reward: 0.8717797887081347.\n",
      "Step: 16300. Mean Reward: 1.0. Std of Reward: 0.0.\n",
      "Step: 16400. Mean Reward: -0.7. Std of Reward: 0.4.\n",
      "Step: 16500. Mean Reward: 1.0. Std of Reward: 0.0.\n",
      "Step: 16600. Mean Reward: -0.3333333333333333. Std of Reward: 0.9428090415820634.\n",
      "Step: 16700. Mean Reward: -0.25. Std of Reward: 0.75.\n",
      "Step: 16800. Mean Reward: -0.3333333333333333. Std of Reward: 0.6236095644623235.\n",
      "Step: 16900. Mean Reward: 0.25. Std of Reward: 0.75.\n",
      "Step: 17000. Mean Reward: -0.5. Std of Reward: 0.5.\n",
      "Saved Model\n",
      "Step: 17100. Mean Reward: -0.125. Std of Reward: 0.8926785535678563.\n",
      "Step: 17200. Mean Reward: 1.0. Std of Reward: 0.0.\n",
      "Step: 17300. Mean Reward: -0.3333333333333333. Std of Reward: 0.6236095644623235.\n",
      "Step: 17400. Mean Reward: 1.0. Std of Reward: 0.0.\n",
      "Step: 17500. Mean Reward: -0.625. Std of Reward: 0.414578098794425.\n",
      "Step: 17600. Mean Reward: -0.3333333333333333. Std of Reward: 0.9428090415820634.\n",
      "Step: 17700. Mean Reward: 1.0. Std of Reward: 0.0.\n",
      "Step: 17800. Mean Reward: -0.2. Std of Reward: 0.9797958971132713.\n",
      "Step: 17900. Mean Reward: 0.5. Std of Reward: 0.0.\n",
      "Step: 18000. Mean Reward: 0.0. Std of Reward: 0.7905694150420949.\n",
      "Saved Model\n",
      "Step: 18100. Mean Reward: -0.3333333333333333. Std of Reward: 0.9428090415820634.\n",
      "Step: 18200. Mean Reward: -0.7. Std of Reward: 0.6.\n",
      "Step: 18300. Mean Reward: -0.5. Std of Reward: 0.408248290463863.\n",
      "Step: 18400. Mean Reward: 1.0. Std of Reward: 0.0.\n",
      "Step: 18500. Mean Reward: 0.0. Std of Reward: 0.5.\n",
      "Step: 18600. Mean Reward: -0.3333333333333333. Std of Reward: 0.6236095644623235.\n",
      "Step: 18700. Mean Reward: -0.3333333333333333. Std of Reward: 0.6236095644623235.\n",
      "Step: 18800. Mean Reward: -0.3333333333333333. Std of Reward: 0.23570226039551584.\n",
      "Step: 18900. Mean Reward: 0.16666666666666666. Std of Reward: 0.8498365855987975.\n",
      "Step: 19000. Mean Reward: -0.625. Std of Reward: 0.649519052838329.\n",
      "Saved Model\n",
      "Step: 19100. Mean Reward: -0.625. Std of Reward: 0.649519052838329.\n",
      "Step: 19200. Mean Reward: -0.5. Std of Reward: 0.4472135954999579.\n",
      "Step: 19300. Mean Reward: -0.25. Std of Reward: 0.75.\n",
      "Step: 19400. Mean Reward: -0.5. Std of Reward: 0.8660254037844386.\n",
      "Step: 19500. Mean Reward: -0.625. Std of Reward: 0.414578098794425.\n",
      "Step: 19600. Mean Reward: -0.3333333333333333. Std of Reward: 0.23570226039551584.\n",
      "Step: 19700. Mean Reward: 0.5. Std of Reward: 0.0.\n",
      "Step: 19800. Mean Reward: 0.375. Std of Reward: 0.960143218483576.\n",
      "Step: 19900. Mean Reward: -0.3333333333333333. Std of Reward: 0.6236095644623235.\n",
      "Step: 20000. Mean Reward: -0.3333333333333333. Std of Reward: 0.6236095644623235.\n",
      "Saved Model\n",
      "Step: 20100. Mean Reward: -0.7. Std of Reward: 0.6.\n",
      "Step: 20200. Mean Reward: 0.5. Std of Reward: 0.0.\n",
      "Step: 20300. Mean Reward: -0.3333333333333333. Std of Reward: 0.9428090415820634.\n",
      "Step: 20400. Mean Reward: 0.5. Std of Reward: 0.0.\n",
      "Step: 20500. Mean Reward: 0.25. Std of Reward: 0.75.\n",
      "Step: 20600. Mean Reward: -0.5. Std of Reward: 0.7071067811865476.\n",
      "Step: 20700. Mean Reward: 1.0. Std of Reward: 0.0.\n",
      "Step: 20800. Mean Reward: 1.0. Std of Reward: 0.0.\n",
      "Step: 20900. Mean Reward: 0.0. Std of Reward: 1.0.\n",
      "Step: 21000. Mean Reward: -0.25. Std of Reward: 0.25.\n",
      "Saved Model\n",
      "Step: 21100. Mean Reward: -0.3333333333333333. Std of Reward: 0.6236095644623235.\n",
      "Step: 21200. Mean Reward: 1.0. Std of Reward: 0.0.\n",
      "Step: 21300. Mean Reward: 0.0. Std of Reward: 1.0.\n",
      "Step: 21400. Mean Reward: -0.5. Std of Reward: 0.8660254037844386.\n",
      "Step: 21500. Mean Reward: 0.0. Std of Reward: 0.5.\n",
      "Step: 21600. Mean Reward: -0.125. Std of Reward: 0.649519052838329.\n",
      "Step: 21700. Mean Reward: 0.5. Std of Reward: 0.0.\n",
      "Step: 21800. Mean Reward: 0.0. Std of Reward: 1.0.\n",
      "Step: 21900. Mean Reward: 1.0. Std of Reward: 0.0.\n",
      "Step: 22000. Mean Reward: -0.3333333333333333. Std of Reward: 0.9428090415820634.\n",
      "Saved Model\n",
      "Step: 22100. Mean Reward: -0.8571428571428571. Std of Reward: 0.3499271061118826.\n",
      "Step: 22200. Mean Reward: 0.5. Std of Reward: 0.0.\n",
      "Step: 22300. Mean Reward: 0.0. Std of Reward: 0.5.\n",
      "Step: 22400. Mean Reward: -0.5. Std of Reward: 0.8660254037844386.\n",
      "Step: 22500. Mean Reward: -0.25. Std of Reward: 0.75.\n",
      "Step: 22600. Mean Reward: 1.0. Std of Reward: 0.0.\n",
      "Step: 22700. Mean Reward: -0.625. Std of Reward: 0.649519052838329.\n",
      "Step: 22800. Mean Reward: -0.5. Std of Reward: 0.6123724356957945.\n",
      "Step: 22900. Mean Reward: 1.0. Std of Reward: 0.0.\n",
      "Step: 23000. Mean Reward: 1.0. Std of Reward: 0.0.\n",
      "Saved Model\n",
      "Step: 23100. Mean Reward: 0.5. Std of Reward: 0.0.\n",
      "Step: 23200. Mean Reward: -0.5. Std of Reward: 0.6123724356957945.\n",
      "Step: 23300. Mean Reward: -0.5. Std of Reward: 0.8660254037844386.\n",
      "Step: 23400. Mean Reward: -0.4. Std of Reward: 0.7348469228349533.\n",
      "Step: 23500. Mean Reward: 0.0. Std of Reward: 0.5.\n",
      "Step: 23600. Mean Reward: 0.25. Std of Reward: 0.25.\n",
      "Step: 23700. Mean Reward: 0.0. Std of Reward: 0.5.\n",
      "Step: 23800. Mean Reward: -0.3333333333333333. Std of Reward: 0.9428090415820634.\n",
      "Step: 23900. Mean Reward: 0.0. Std of Reward: 1.0.\n",
      "Step: 24000. Mean Reward: 0.0. Std of Reward: 1.0.\n",
      "Saved Model\n",
      "Step: 24100. Mean Reward: -0.6. Std of Reward: 0.5830951894845302.\n",
      "Step: 24200. Mean Reward: 0.5. Std of Reward: 0.0.\n",
      "Step: 24300. Mean Reward: 1.0. Std of Reward: 0.408248290463863.\n",
      "Step: 24400. Mean Reward: 1.0. Std of Reward: 0.0.\n",
      "Step: 24500. Mean Reward: -0.3333333333333333. Std of Reward: 0.9428090415820634.\n",
      "Step: 24600. Mean Reward: -0.25. Std of Reward: 0.75.\n",
      "Step: 24700. Mean Reward: -0.7. Std of Reward: 0.4.\n",
      "Step: 24800. Mean Reward: 1.0. Std of Reward: 0.0.\n",
      "Step: 24900. Mean Reward: -0.5. Std of Reward: 0.8660254037844386.\n",
      "Step: 25000. Mean Reward: -0.3333333333333333. Std of Reward: 0.6236095644623235.\n",
      "Saved Model\n",
      "Step: 25100. Mean Reward: 0.5. Std of Reward: 0.0.\n",
      "Step: 25200. Mean Reward: -0.125. Std of Reward: 0.739509972887452.\n",
      "Step: 25300. Mean Reward: -0.5. Std of Reward: 0.7071067811865476.\n",
      "Step: 25400. Mean Reward: -0.3. Std of Reward: 0.8717797887081347.\n",
      "Step: 25500. Mean Reward: -0.8333333333333334. Std of Reward: 0.37267799624996495.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 25600. Mean Reward: -0.625. Std of Reward: 0.414578098794425.\n",
      "Step: 25700. Mean Reward: 1.0. Std of Reward: 0.0.\n",
      "Step: 25800. Mean Reward: -0.6. Std of Reward: 0.8000000000000002.\n",
      "Step: 25900. Mean Reward: 1.25. Std of Reward: 0.75.\n",
      "Step: 26000. Mean Reward: -0.625. Std of Reward: 0.649519052838329.\n",
      "Saved Model\n",
      "Step: 26100. Mean Reward: 0.25. Std of Reward: 0.75.\n",
      "Step: 26200. Mean Reward: -0.8333333333333334. Std of Reward: 0.23570226039551584.\n",
      "Step: 26300. Mean Reward: -0.875. Std of Reward: 0.33071891388307384.\n",
      "Step: 26400. Mean Reward: -0.625. Std of Reward: 0.414578098794425.\n",
      "Step: 26500. Mean Reward: -0.25. Std of Reward: 0.5590169943749475.\n",
      "Step: 26600. Mean Reward: -0.25. Std of Reward: 0.25.\n",
      "Step: 26700. Mean Reward: 0.25. Std of Reward: 0.25.\n",
      "Step: 26800. Mean Reward: -0.5. Std of Reward: 0.8660254037844386.\n",
      "Step: 26900. Mean Reward: -0.3333333333333333. Std of Reward: 0.6236095644623235.\n",
      "Step: 27000. Mean Reward: 0.0. Std of Reward: 0.816496580927726.\n",
      "Saved Model\n",
      "Step: 27100. Mean Reward: 1.0. Std of Reward: 0.0.\n",
      "Step: 27200. Mean Reward: 1.0. Std of Reward: 0.0.\n",
      "Step: 27300. Mean Reward: -0.25. Std of Reward: 0.5590169943749475.\n",
      "Step: 27400. Mean Reward: -0.3333333333333333. Std of Reward: 0.9428090415820634.\n",
      "Step: 27500. Mean Reward: 1.0. Std of Reward: 0.0.\n",
      "Step: 27600. Mean Reward: -0.6. Std of Reward: 0.5830951894845302.\n",
      "Step: 27700. Mean Reward: -0.3333333333333333. Std of Reward: 0.6236095644623235.\n",
      "Step: 27800. Mean Reward: 0.5. Std of Reward: 0.0.\n",
      "Step: 27900. Mean Reward: 0.3333333333333333. Std of Reward: 0.9428090415820634.\n",
      "Step: 28000. Mean Reward: -0.5. Std of Reward: 0.408248290463863.\n",
      "Saved Model\n",
      "Step: 28100. Mean Reward: 0.0. Std of Reward: 0.5.\n",
      "Step: 28200. Mean Reward: 0.0. Std of Reward: 0.5.\n",
      "Step: 28300. Mean Reward: 0.0. Std of Reward: 0.5.\n",
      "Step: 28400. Mean Reward: 1.0. Std of Reward: 0.0.\n",
      "Step: 28500. Mean Reward: 0.0. Std of Reward: 0.5.\n",
      "Step: 28600. Mean Reward: 1.0. Std of Reward: 0.0.\n",
      "Step: 28700. Mean Reward: -0.6666666666666666. Std of Reward: 0.23570226039551584.\n",
      "Step: 28800. Mean Reward: -0.5. Std of Reward: 0.7071067811865476.\n",
      "Step: 28900. Mean Reward: 1.0. Std of Reward: 0.0.\n",
      "Step: 29000. Mean Reward: -0.7. Std of Reward: 0.4.\n",
      "Saved Model\n",
      "Step: 29100. Mean Reward: -0.625. Std of Reward: 0.649519052838329.\n",
      "Step: 29200. Mean Reward: 0.0. Std of Reward: 1.0.\n",
      "Step: 29300. Mean Reward: 1.0. Std of Reward: 0.0.\n",
      "Step: 29400. Mean Reward: -0.3333333333333333. Std of Reward: 0.9428090415820634.\n",
      "Step: 29500. Mean Reward: 1.0. Std of Reward: 0.0.\n",
      "Step: 29600. Mean Reward: 0.0. Std of Reward: 1.0.\n",
      "Step: 29700. Mean Reward: 0.16666666666666666. Std of Reward: 0.4714045207910317.\n",
      "Step: 29800. Mean Reward: -0.625. Std of Reward: 0.649519052838329.\n",
      "Step: 29900. Mean Reward: -0.625. Std of Reward: 0.414578098794425.\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "if curriculum_file == \"None\":\n",
    "    curriculum_file = None\n",
    "\n",
    "\n",
    "def get_progress():\n",
    "    if curriculum_file is not None:\n",
    "        if env._curriculum.measure_type == \"progress\":\n",
    "            return steps / max_steps\n",
    "        elif env._curriculum.measure_type == \"reward\":\n",
    "            return last_reward\n",
    "        else:\n",
    "            return None\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Create the Tensorflow model graph\n",
    "ppo_model = create_agent_model(env, lr=learning_rate,\n",
    "                               h_size=hidden_units, epsilon=epsilon,\n",
    "                               beta=beta, max_step=max_steps, \n",
    "                               normalize=normalize, num_layers=num_layers)\n",
    "\n",
    "is_continuous = (env.brains[brain_name].action_space_type == \"continuous\")\n",
    "use_observations = (env.brains[brain_name].number_observations > 0)\n",
    "use_states = (env.brains[brain_name].state_space_size > 0)\n",
    "\n",
    "model_path = './models/{}'.format(run_path)\n",
    "summary_path = './summaries/{}'.format(run_path)\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "\n",
    "if not os.path.exists(summary_path):\n",
    "    os.makedirs(summary_path)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Instantiate model parameters\n",
    "    if load_model:\n",
    "        print('Loading Model...')\n",
    "        ckpt = tf.train.get_checkpoint_state(model_path)\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    else:\n",
    "        sess.run(init)\n",
    "    steps, last_reward = sess.run([ppo_model.global_step, ppo_model.last_reward])    \n",
    "    summary_writer = tf.summary.FileWriter(summary_path)\n",
    "    info = env.reset(train_mode=train_model, progress=get_progress())[brain_name]\n",
    "    trainer = Trainer(ppo_model, sess, info, is_continuous, use_observations, use_states, train_model)\n",
    "    if train_model:\n",
    "        trainer.write_text(summary_writer, 'Hyperparameters', hyperparameter_dict, steps)\n",
    "    while steps <= max_steps:\n",
    "        if env.global_done:\n",
    "            info = env.reset(train_mode=train_model, progress=get_progress())[brain_name]\n",
    "        # Decide and take an action\n",
    "        new_info = trainer.take_action(info, env, brain_name, steps, normalize)\n",
    "        info = new_info\n",
    "        trainer.process_experiences(info, time_horizon, gamma, lambd)\n",
    "        if len(trainer.training_buffer['actions']) > buffer_size and train_model:\n",
    "            # Perform gradient descent with experience buffer\n",
    "            trainer.update_model(batch_size, num_epoch)\n",
    "        if steps % summary_freq == 0 and steps != 0 and train_model:\n",
    "            # Write training statistics to tensorboard.\n",
    "            trainer.write_summary(summary_writer, steps, env._curriculum.lesson_number)\n",
    "        if steps % save_freq == 0 and steps != 0 and train_model:\n",
    "            # Save Tensorflow model\n",
    "            save_model(sess, model_path=model_path, steps=steps, saver=saver)\n",
    "        steps += 1\n",
    "        sess.run(ppo_model.increment_step)\n",
    "        if len(trainer.stats['cumulative_reward']) > 0:\n",
    "            mean_reward = np.mean(trainer.stats['cumulative_reward'])\n",
    "            sess.run(ppo_model.update_reward, feed_dict={ppo_model.new_reward: mean_reward})\n",
    "            last_reward = sess.run(ppo_model.last_reward)\n",
    "    # Final save Tensorflow model\n",
    "    if steps != 0 and train_model:\n",
    "        save_model(sess, model_path=model_path, steps=steps, saver=saver)\n",
    "env.close()\n",
    "export_graph(model_path, env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export the trained Tensorflow graph\n",
    "Once the model has been trained and saved, we can export it as a .bytes file which Unity can embed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "export_graph(model_path, env_name)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
